{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import load_model\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import pickle as pkl\n",
    "import matplotlib\n",
    "rootPathList=['data']\n",
    "CHECK_DRIVERS_NUMBER = 3\n",
    "drivers_to_files=pd.read_csv('driver_imgs_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_data(rootPathList):\n",
    "    print('开始重新恢复初始数据集。。。。。。')\n",
    "    for i,rootPath in enumerate(rootPathList):           \n",
    "        for  filePath in (os.listdir(rootPath + '/valid')):\n",
    "            files = os.listdir(rootPath + '/valid/'+filePath)\n",
    "            i = filePath[-1]\n",
    "            for file in files:\n",
    "                targetFile = (rootPath + '/train' + '/c%s/' + file) % (i )\n",
    "                sourceFile = (rootPath + '/valid' + '/c%s/'+ file) % (i )\n",
    "                if (not os.path.exists(targetFile)) and os.path.exists(sourceFile):\n",
    "                    os.rename(sourceFile, targetFile)\n",
    "    print('数据已经恢复完毕，可以开始重新挑选数据集')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_train_data(num,rootPathList,CHECK_DRIVERS_NUMBER):\n",
    "    print('开始挑选本次训练的数据集')\n",
    "    for i,rootPath in enumerate(rootPathList): \n",
    "        check_drivers = drivers_to_files.drop_duplicates(['subject'])['subject'].sample(n=CHECK_DRIVERS_NUMBER, random_state=(num+209))\n",
    "        print('对于第 %d次训练,抽取的验证集司机编号为：'% num)\n",
    "        print(check_drivers)\n",
    "        for driver in check_drivers:\n",
    "            check_drivers_dataFrame = drivers_to_files.loc[drivers_to_files['subject'] == driver]\n",
    "            for indexs in check_drivers_dataFrame.index:\n",
    "                filePath = check_drivers_dataFrame.loc[indexs]\n",
    "                sourceFile = (rootPath + '/train' + '/' + filePath['classname'] + '/' + filePath['img'])\n",
    "                targetFile = (rootPath + '/valid' + '/' + filePath['classname'] + '/' + filePath['img'])\n",
    "                if (not os.path.exists(targetFile)) and os.path.exists(sourceFile):\n",
    "                    os.rename(sourceFile, targetFile)\n",
    "    print('数据集挑选完毕，可以开始训练过程')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始重新恢复初始数据集。。。。。。\n",
      "数据已经恢复完毕，可以开始重新挑选数据集\n",
      "开始挑选本次训练的数据集\n",
      "对于第 0次训练,抽取的验证集司机编号为：\n",
      "17778    p061\n",
      "16984    p056\n",
      "13523    p049\n",
      "Name: subject, dtype: object\n",
      "数据集挑选完毕，可以开始训练过程\n"
     ]
    }
   ],
   "source": [
    "recover_data(rootPathList)\n",
    "pick_train_data(0,rootPathList,CHECK_DRIVERS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageDataGenerator_select(funtion_name):\n",
    "    datagen = None\n",
    "    imgesize = None\n",
    "    if funtion_name == 'DenseNet201':\n",
    "        imgesize = (224,224)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=densenet.preprocess_input)\n",
    "    if funtion_name == 'InceptionResNetV2':\n",
    "        imgesize = (299,299)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=inception_resnet_v2.preprocess_input)\n",
    "    if funtion_name == 'InceptionV3':\n",
    "        imgesize = (299,299)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=inception_v3.preprocess_input)\n",
    "    if funtion_name == 'Xception':\n",
    "        imgesize = (299,299)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=xception.preprocess_input)\n",
    "    if funtion_name == 'ResNet50':\n",
    "        imgesize = (224,224)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=resnet50.preprocess_input)\n",
    "    if funtion_name == 'VGG19':\n",
    "        imgesize = (224,224)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=vgg19.preprocess_input)\n",
    "    if funtion_name == 'VGG16':\n",
    "        imgesize = (224,224)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=vgg16.preprocess_input)\n",
    "    if funtion_name == 'NASNetLarge':\n",
    "        imgesize = (331,331)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=nasnet.preprocess_input)\n",
    "    if funtion_name == 'NASNetMobile':\n",
    "        imgesize = (224,224)\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=nasnet.preprocess_input)\n",
    "    return datagen,imgesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_class = 0\n",
    "rootFilePath ='data'\n",
    "funtion_name = 'VGG16'\n",
    "\n",
    "datagen,imageSize = ImageDataGenerator_select('VGG16')\n",
    "\n",
    "train_generator_label_batch_size = 32\n",
    "train_generator_nolabel_batch_size = 32\n",
    "valid_generator_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, batch_size, image_size, datagen, steps_per_epoch, valid_steps_per_epoch, alpha=0.5):\n",
    "\n",
    "        self.label_batch_size = int(batch_size*alpha)\n",
    "        self.nolabel_batch_size = batch_size - self.label_batch_size\n",
    "\n",
    "        self.train_generator_label = datagen.flow_from_directory(\n",
    "            rootFilePath+'/train',\n",
    "            target_size=image_size,\n",
    "            batch_size=self.label_batch_size,\n",
    "            class_mode='sparse')\n",
    "        self.train_generator_nolabel = datagen.flow_from_directory(\n",
    "            rootFilePath + '/test1',\n",
    "            target_size=image_size,\n",
    "            batch_size=self.nolabel_batch_size,\n",
    "            class_mode='sparse')\n",
    "        self.valid_generator = datagen.flow_from_directory(\n",
    "            rootFilePath + '/valid',\n",
    "            target_size=image_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='sparse')\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch // batch_size * 2\n",
    "        self.valid_steps_per_epoch = self.valid_generator.samples //batch_size\n",
    "        \n",
    "    def test(self):\n",
    "        print (next(self.train_generator_label)[1])\n",
    "        \n",
    "    def get_batches(self):\n",
    "        for i in range(self.steps_per_epoch):\n",
    "            train_label = next(self.train_generator_label)\n",
    "            train_nolabel = next(self.train_generator_nolabel)\n",
    "\n",
    "            train_label_x = train_label[0]\n",
    "            train_label_y = train_label[1].reshape(-1, 1)\n",
    "\n",
    "            train_nolabel_x =train_nolabel[0]\n",
    "            train_nolabel_y = np.zeros((train_nolabel[0].shape[0], 1),dtype = 'int64')\n",
    "\n",
    "            train_x_batch = np.concatenate((train_label_x, train_nolabel_x))\n",
    "            train_y_batch = np.concatenate((train_label_y, train_nolabel_y))\n",
    "            \n",
    "            train_label_mask = np.ones_like(train_label_y, dtype='int64')\n",
    "            train_nolabel_mask = np.zeros_like(train_nolabel_y, dtype ='int64')\n",
    "            label_mask = np.concatenate((train_label_mask, train_nolabel_mask))\n",
    "\n",
    "            train_x_batch, train_y_batch, label_mask = shuffle(train_x_batch, train_y_batch, label_mask)\n",
    "\n",
    "            yield train_x_batch, train_y_batch, label_mask\n",
    "            \n",
    "    def get_valid_batches(self):       \n",
    "        for i in range(self.valid_steps_per_epoch):\n",
    "            valid_batch = self.valid_generator.next()\n",
    "            valid_x_batch = valid_batch[0]\n",
    "            valid_y_batch = (valid_batch[1]).reshape(-1, 1)\n",
    "            yield valid_x_batch, valid_y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(num , train_generator_label, train_generator_nolabel,nolabel_batch_size,label_batch_size):\n",
    "    \n",
    "    for i in range(num):\n",
    "\n",
    "        train_label = train_generator_label.next()\n",
    "        train_nolabel = train_generator_nolabel.next()\n",
    "\n",
    "        train_nolabel_y = np.zeros((nolabel_batch_size, 1))\n",
    "        train_nolabel_y.dtype ='int64'\n",
    "        train_label_y =np.argmax(train_label[1][:label_batch_size],axis = 1).reshape(-1,1)\n",
    "       \n",
    "\n",
    "        train_x_batch = np.concatenate((train_label[0], train_nolabel[0]))\n",
    "        train_y_batch = np.concatenate((train_label_y, train_nolabel_y))\n",
    "        \n",
    "        \n",
    "        train_label_mask = np.ones_like(train_label_y)\n",
    "        train_nolabel_mask = np.zeros_like(train_nolabel_y)\n",
    "        \n",
    "        label_mask = np.concatenate((train_label_mask, train_nolabel_mask))\n",
    "\n",
    "        train_x_batch, train_y_batch, label_mask = shuffle(train_x_batch, train_y_batch,label_mask)\n",
    " \n",
    "        label_mask.dtype = 'int64'\n",
    "\n",
    "        yield train_x_batch, train_y_batch, label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_batch(num , valid_generator):\n",
    "\n",
    "    for i in range(num):\n",
    "        valid_batch = valid_generator.next()\n",
    "        valid_x_batch = valid_batch[0]\n",
    "        valid_y_batch = np.argmax(valid_batch[1],axis = 1).reshape(-1,1)\n",
    "        yield valid_x_batch, valid_y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_size = (224,224,3)\n",
    "z_size = 200\n",
    "learning_rate = 0.0003\n",
    "batch_size = 128\n",
    "imageSize = (224,224)\n",
    "\n",
    "\n",
    "    \n",
    "# label_batch_size = batch_size//2\n",
    "# nobel_batch_size = batch_size - label_batch_size\n",
    "\n",
    "# train_generator_label = datagen.flow_from_directory(\n",
    "#             rootFilePath+'/train',  # this is the target directory\n",
    "#             target_size=imageSize,  # all images will be resized to 150x150\n",
    "#             batch_size=label_batch_size,\n",
    "#             class_mode='categorical')\n",
    "\n",
    "# train_generator_nolabel = datagen.flow_from_directory(\n",
    "#         rootFilePath+'/test1',  # this is the target directory\n",
    "#         target_size=imageSize,  # all images will be resized to 150x150\n",
    "#         batch_size=nobel_batch_size,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "# valid_generator = datagen.flow_from_directory(\n",
    "#         rootFilePath+'/valid',  # this is the target directory\n",
    "#         target_size=imageSize,  # all images will be resized to 150x150\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "# steps_per_epoch = int(train_generator_label.samples/batch_size/2)\n",
    "# valid_steps_per_epoch = int(valid_generator.samples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    y = tf.placeholder(tf.int32, (None), name='y')\n",
    "    label_mask = tf.placeholder(tf.int32, (None), name='label_mask')\n",
    "    \n",
    "    return inputs_real, inputs_z, y, label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, output_dim, reuse=False, alpha=0.2, training=True, size_mult=128):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)\n",
    "        # Reshape it to start the convolutional stack\n",
    "        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))\n",
    "        x1 = tf.layers.batch_normalization(x1, training=training)\n",
    "        x1 = tf.maximum(alpha * x1, x1)\n",
    "        \n",
    "        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding='same')\n",
    "        x2 = tf.layers.batch_normalization(x2, training=training)\n",
    "        x2 = tf.maximum(alpha * x2, x2)\n",
    "        \n",
    "        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='same')\n",
    "        x3 = tf.layers.batch_normalization(x3, training=training)\n",
    "        x3 = tf.maximum(alpha * x3, x3)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2, drop_rate=0., num_classes=10, size_mult=64):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        x = tf.layers.dropout(x, rate=drop_rate/2.5)\n",
    "        \n",
    "        # Input layer is 32x32x3\n",
    "        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')\n",
    "        relu1 = tf.maximum(alpha * x1, x1)\n",
    "        relu1 = tf.layers.dropout(relu1, rate=drop_rate)\n",
    "        \n",
    "        x2 = tf.layers.conv2d(relu1, size_mult, 3, strides=2, padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(x2, training=True)\n",
    "        relu2 = tf.maximum(alpha * x2, x2)\n",
    "        \n",
    "        \n",
    "        x3 = tf.layers.conv2d(relu2, size_mult, 3, strides=2, padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(x3, training=True)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "        relu3 = tf.layers.dropout(relu3, rate=drop_rate)\n",
    "        \n",
    "        x4 = tf.layers.conv2d(relu3, 2 * size_mult, 3, strides=1, padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(x4, training=True)\n",
    "        relu4 = tf.maximum(alpha * bn4, bn4)\n",
    "        \n",
    "        x5 = tf.layers.conv2d(relu4, 2 * size_mult, 3, strides=1, padding='same')\n",
    "        bn5 = tf.layers.batch_normalization(x5, training=True)\n",
    "        relu5 = tf.maximum(alpha * bn5, bn5)\n",
    "        \n",
    "        x6 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=2, padding='same')\n",
    "        bn6 = tf.layers.batch_normalization(x6, training=True)\n",
    "        relu6 = tf.maximum(alpha * bn6, bn6)\n",
    "        relu6 = tf.layers.dropout(relu6, rate=drop_rate)\n",
    "        \n",
    "        x7 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=1, padding='valid')\n",
    "        # Don't use bn on this layer, because bn would set the mean of each feature\n",
    "        # to the bn mu parameter.\n",
    "        # This layer is used for the feature matching loss, which only works if\n",
    "        # the means can be different when the discriminator is run on the data than\n",
    "        # when the discriminator is run on the generator samples.\n",
    "        relu7 = tf.maximum(alpha * x7, x7)\n",
    "        \n",
    "        # Flatten it by global average pooling\n",
    "        features = tf.reduce_mean(relu7, (1, 2))\n",
    "        \n",
    "        # Set class_logits to be the inputs to a softmax distribution over the different classes\n",
    "        class_logits = tf.layers.dense(features, num_classes + extra_class)\n",
    "        \n",
    "        \n",
    "        # Set gan_logits such that P(input is real | input) = sigmoid(gan_logits).\n",
    "        # Keep in mind that class_logits gives you the probability distribution over all the real\n",
    "        # classes and the fake class. You need to work out how to transform this multiclass softmax\n",
    "        # distribution into a binary real-vs-fake decision that can be described with a sigmoid.\n",
    "        # Numerical stability is very important.\n",
    "        # You'll probably need to use this numerical stability trick:\n",
    "        # log sum_i exp a_i = m + log sum_i exp(a_i - m).\n",
    "        # This is numerically stable when m = max_i a_i.\n",
    "        # (It helps to think about what goes wrong when...\n",
    "        #   1. One value of a_i is very large\n",
    "        #   2. All the values of a_i are very negative\n",
    "        # This trick and this value of m fix both those cases, but the naive implementation and\n",
    "        # other values of m encounter various problems)\n",
    "        \n",
    "        if extra_class:\n",
    "            real_class_logits, fake_class_logits = tf.split(class_logits, [num_classes, 1], 1)\n",
    "            assert fake_class_logits.get_shape()[1] == 1, fake_class_logits.get_shape()\n",
    "            fake_class_logits = tf.squeeze(fake_class_logits)\n",
    "        else:\n",
    "            real_class_logits = class_logits\n",
    "            fake_class_logits = 0.\n",
    "        \n",
    "        mx = tf.reduce_max(real_class_logits, 1, keep_dims=True)\n",
    "        stable_real_class_logits = real_class_logits - mx\n",
    "\n",
    "        gan_logits = tf.log(tf.reduce_sum(tf.exp(stable_real_class_logits), 1)) + tf.squeeze(mx) - fake_class_logits\n",
    "        \n",
    "        out = tf.nn.softmax(class_logits)\n",
    "        \n",
    "        return out, class_logits, gan_logits, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_dim, y, num_classes, label_mask, alpha=0.2, drop_rate=0.):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input\n",
    "    :param output_dim: The number of channels in the output image\n",
    "    :param y: Integer class labels\n",
    "    :param num_classes: The number of classes\n",
    "    :param alpha: The slope of the left half of leaky ReLU activation\n",
    "    :param drop_rate: The probability of dropping a hidden unit\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # These numbers multiply the size of each layer of the generator and the discriminator,\n",
    "    # respectively. You can reduce them to run your code faster for debugging purposes.\n",
    "    g_size_mult = 32\n",
    "    d_size_mult = 64\n",
    "    \n",
    "    # Here we run the generator and the discriminator\n",
    "    g_model = generator(input_z, output_dim, alpha=alpha, size_mult=g_size_mult)\n",
    "    d_on_data = discriminator(input_real, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)\n",
    "    d_model_real, class_logits_on_data, gan_logits_on_data, data_features = d_on_data\n",
    "    d_on_samples = discriminator(g_model, reuse=True, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)\n",
    "    d_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = d_on_samples\n",
    "    \n",
    "    \n",
    "    # Here we compute `d_loss`, the loss for the discriminator.\n",
    "    # This should combine two different losses:\n",
    "    #  1. The loss for the GAN problem, where we minimize the cross-entropy for the binary\n",
    "    #     real-vs-fake classification problem.\n",
    "    #  2. The loss for the SVHN digit classification problem, where we minimize the cross-entropy\n",
    "    #     for the multi-class softmax. For this one we use the labels. Don't forget to ignore\n",
    "    #     use `label_mask` to ignore the examples that we are pretending are unlabeled for the\n",
    "    #     semi-supervised learning problem.\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits_on_data,\n",
    "                                                labels=tf.ones_like(gan_logits_on_data)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits_on_samples,\n",
    "                                                labels=tf.zeros_like(gan_logits_on_samples)))\n",
    "    y = tf.squeeze(y)\n",
    "    class_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=class_logits_on_data,\n",
    "                                                                  labels=tf.one_hot(y, num_classes + extra_class,\n",
    "                                                                                    dtype=tf.float32))\n",
    "    class_cross_entropy = tf.squeeze(class_cross_entropy)\n",
    "    label_mask = tf.squeeze(tf.to_float(label_mask))\n",
    "    d_loss_class = tf.reduce_sum(label_mask * class_cross_entropy) / tf.maximum(1., tf.reduce_sum(label_mask))\n",
    "    d_loss = d_loss_class + d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\n",
    "    # This loss consists of minimizing the absolute difference between the expected features\n",
    "    # on the data and the expected features on the generated samples.\n",
    "    # This loss works better for semi-supervised learning than the tradition GAN losses.\n",
    "    data_moments = tf.reduce_mean(data_features, axis=0)\n",
    "    sample_moments = tf.reduce_mean(sample_features, axis=0)\n",
    "    g_loss = tf.reduce_mean(tf.abs(data_moments - sample_moments))\n",
    "\n",
    "    pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)\n",
    "    eq = tf.equal(tf.squeeze(y), pred_class)\n",
    "    correct = tf.reduce_sum(tf.to_float(eq))\n",
    "    masked_correct = tf.reduce_sum(label_mask * tf.to_float(eq))\n",
    "    \n",
    "    return d_loss, g_loss, correct, masked_correct, g_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and biases to update. Get them separately for the discriminator and the generator\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    for t in t_vars:\n",
    "        assert t in d_vars or t in g_vars\n",
    "\n",
    "    # Minimize both players' costs simultaneously\n",
    "    d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "    g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "    shrink_lr = tf.assign(learning_rate, learning_rate * 0.9)\n",
    "    \n",
    "    return d_train_opt, g_train_opt, shrink_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    \"\"\"\n",
    "    A GAN model.\n",
    "    :param real_size: The shape of the real data.\n",
    "    :param z_size: The number of entries in the z code vector.\n",
    "    :param learnin_rate: The learning rate to use for Adam.\n",
    "    :param num_classes: The number of classes to recognize.\n",
    "    :param alpha: The slope of the left half of the leaky ReLU activation\n",
    "    :param beta1: The beta1 parameter for Adam.\n",
    "    \"\"\"\n",
    "    def __init__(self, real_size, z_size, learning_rate, num_classes=10, alpha=0.2, beta1=0.5):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.learning_rate = tf.Variable(learning_rate, trainable=False)\n",
    "        self.input_real, self.input_z, self.y, self.label_mask = model_inputs(real_size, z_size)\n",
    "        self.drop_rate = tf.placeholder_with_default(.5, (), \"drop_rate\")\n",
    "        \n",
    "        loss_results = model_loss(self.input_real, self.input_z,\n",
    "                                              real_size[2], self.y, num_classes, label_mask=self.label_mask,\n",
    "                                                                          alpha=0.2,\n",
    "                                                           drop_rate=self.drop_rate)\n",
    "        self.d_loss, self.g_loss, self.correct, self.masked_correct, self.samples = loss_results\n",
    "        \n",
    "        self.d_opt, self.g_opt, self.shrink_lr = model_opt(self.d_loss, self.g_loss, self.learning_rate, beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples, nrows, ncols, figsize=(5,5)):\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, \n",
    "                             sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.axis('off')\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.set_adjustable('box-forced')\n",
    "        im = ax.imshow(img)\n",
    "   \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,epochs,dataSet,batch_size, figsize=(5,5)):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sample_z = np.random.normal(0, 1, size=(50, z_size))\n",
    "\n",
    "    samples, train_accuracies, test_accuracies = [], [], []\n",
    "    steps = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(epochs):\n",
    "            print(\"Epoch\",e)\n",
    "            \n",
    "            t1e = time.time()\n",
    "            num_examples = 0\n",
    "            num_correct = 0\n",
    "#             for x, y, label_mask in dataset.batches(batch_size):\n",
    "            \n",
    "            for x, y, label_mask in dataSet.get_batches():\n",
    "                \n",
    "                assert 'int' in str(y.dtype)\n",
    "                steps += 1\n",
    "                num_examples += label_mask.sum()\n",
    "\n",
    "                # Sample random noise for G\n",
    "                batch_z = np.random.normal(0, 1, size=(batch_size, z_size))\n",
    "\n",
    "                # Run optimizers\n",
    "                t1 = time.time()\n",
    "                _, _, correct = sess.run([net.d_opt, net.g_opt, net.masked_correct],\n",
    "                                         feed_dict={net.input_real: x, net.input_z: batch_z,\n",
    "                                                    net.y : y, net.label_mask : label_mask})\n",
    "                t2 = time.time()\n",
    "                num_correct += correct\n",
    "            sess.run([net.shrink_lr])\n",
    "            \n",
    "            \n",
    "            train_accuracy = num_correct / float(num_examples)\n",
    "            \n",
    "            print(\"\\t\\tClassifier train accuracy: \", train_accuracy)\n",
    "            \n",
    "            num_examples = 0\n",
    "            num_correct = 0\n",
    "            for x, y in dataSet.get_valid_batches():\n",
    "                assert 'int' in str(y.dtype)\n",
    "                num_examples += x.shape[0]\n",
    "\n",
    "                correct, = sess.run([net.correct], feed_dict={net.input_real: x,\n",
    "                                                   net.y : y,\n",
    "                                                   net.drop_rate: 0.})\n",
    "                num_correct += correct\n",
    "            \n",
    "            test_accuracy = num_correct / float(num_examples)\n",
    "            print(\"\\t\\tClassifier test accuracy\", test_accuracy)\n",
    "            print(\"\\t\\tStep time: \", t2 - t1)\n",
    "            t2e = time.time()\n",
    "            print(\"\\t\\tEpoch time: \", t2e - t1e)\n",
    "            \n",
    "            \n",
    "            gen_samples = sess.run(\n",
    "                                   net.samples,\n",
    "                                   feed_dict={net.input_z: sample_z})\n",
    "            samples.append(gen_samples)\n",
    "#             _ = view_samples(-1, samples, 5, 10, figsize=figsize)\n",
    "#             plt.show()\n",
    "            \n",
    "            \n",
    "            # Save history of accuracies to view after training\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            \n",
    "\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "    with open('samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    return train_accuracies, test_accuracies, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GAN(real_size, z_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19810 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n",
      "Found 2614 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "s = DataSet(128,(224,224),datagen,20000,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\t\tClassifier train accuracy:  0.3210452402447588\n",
      "\t\tClassifier test accuracy 0.45092518101367657\n",
      "\t\tStep time:  0.4745016098022461\n",
      "\t\tEpoch time:  610.9776084423065\n",
      "Epoch 1\n",
      "\t\tClassifier train accuracy:  0.6909419199518507\n",
      "\t\tClassifier test accuracy 0.6154465004022526\n",
      "\t\tStep time:  0.48914623260498047\n",
      "\t\tEpoch time:  610.2359771728516\n",
      "Epoch 2\n",
      "\t\tClassifier train accuracy:  0.8668873507874411\n",
      "\t\tClassifier test accuracy 0.6568785197103781\n",
      "\t\tStep time:  0.4700751304626465\n",
      "\t\tEpoch time:  609.82488322258\n",
      "Epoch 3\n",
      "\t\tClassifier train accuracy:  0.9328418096097904\n",
      "\t\tClassifier test accuracy 0.7152051488334674\n",
      "\t\tStep time:  0.477447509765625\n",
      "\t\tEpoch time:  610.7562038898468\n",
      "Epoch 4\n",
      "\t\tClassifier train accuracy:  0.960577791152573\n",
      "\t\tClassifier test accuracy 0.7228479485116653\n",
      "\t\tStep time:  0.48784375190734863\n",
      "\t\tEpoch time:  612.0012636184692\n",
      "Epoch 5\n",
      "\t\tClassifier train accuracy:  0.9723643294212058\n",
      "\t\tClassifier test accuracy 0.7168141592920354\n",
      "\t\tStep time:  0.47682666778564453\n",
      "\t\tEpoch time:  611.488926410675\n",
      "Epoch 6\n",
      "\t\tClassifier train accuracy:  0.9787842311164611\n",
      "\t\tClassifier test accuracy 0.7320997586484312\n",
      "\t\tStep time:  0.4722132682800293\n",
      "\t\tEpoch time:  611.9208364486694\n",
      "Epoch 7\n",
      "\t\tClassifier train accuracy:  0.9841007122078443\n",
      "\t\tClassifier test accuracy 0.7240547063555913\n",
      "\t\tStep time:  0.47596263885498047\n",
      "\t\tEpoch time:  611.5384106636047\n",
      "Epoch 8\n",
      "\t\tClassifier train accuracy:  0.989266726853245\n",
      "\t\tClassifier test accuracy 0.7365245374094932\n",
      "\t\tStep time:  0.471205472946167\n",
      "\t\tEpoch time:  611.0209400653839\n",
      "Epoch 9\n",
      "\t\tClassifier train accuracy:  0.9905707693850938\n",
      "\t\tClassifier test accuracy 0.7083668543845535\n",
      "\t\tStep time:  0.4814612865447998\n",
      "\t\tEpoch time:  611.7975256443024\n",
      "Epoch 10\n",
      "\t\tClassifier train accuracy:  0.9918748119169425\n",
      "\t\tClassifier test accuracy 0.7469831053901851\n",
      "\t\tStep time:  0.4723799228668213\n",
      "\t\tEpoch time:  611.0941958427429\n",
      "Epoch 11\n",
      "\t\tClassifier train accuracy:  0.994783829872605\n",
      "\t\tClassifier test accuracy 0.7345132743362832\n",
      "\t\tStep time:  0.4711747169494629\n",
      "\t\tEpoch time:  610.2755010128021\n",
      "Epoch 12\n",
      "\t\tClassifier train accuracy:  0.9940314976426924\n",
      "\t\tClassifier test accuracy 0.7220434432823813\n",
      "\t\tStep time:  0.4738156795501709\n",
      "\t\tEpoch time:  609.9964942932129\n",
      "Epoch 13\n",
      "\t\tClassifier train accuracy:  0.9971912930083258\n",
      "\t\tClassifier test accuracy 0.7341110217216412\n",
      "\t\tStep time:  0.4793813228607178\n",
      "\t\tEpoch time:  611.560530424118\n",
      "Epoch 14\n",
      "\t\tClassifier train accuracy:  0.99724144849032\n",
      "\t\tClassifier test accuracy 0.6902654867256637\n",
      "\t\tStep time:  0.4796755313873291\n",
      "\t\tEpoch time:  611.9579963684082\n",
      "Epoch 15\n",
      "\t\tClassifier train accuracy:  0.9987461129501455\n",
      "\t\tClassifier test accuracy 0.6967015285599356\n",
      "\t\tStep time:  0.4822404384613037\n",
      "\t\tEpoch time:  611.4315476417542\n",
      "Epoch 16\n",
      "\t\tClassifier train accuracy:  0.9988464239141338\n",
      "\t\tClassifier test accuracy 0.7369267900241352\n",
      "\t\tStep time:  0.4756016731262207\n",
      "\t\tEpoch time:  611.0543646812439\n",
      "Epoch 17\n",
      "\t\tClassifier train accuracy:  0.9992476677700872\n",
      "\t\tClassifier test accuracy 0.7043443282381335\n",
      "\t\tStep time:  0.4746420383453369\n",
      "\t\tEpoch time:  610.8024151325226\n",
      "Epoch 18\n",
      "\t\tClassifier train accuracy:  0.9993981342160698\n",
      "\t\tClassifier test accuracy 0.7139983909895414\n",
      "\t\tStep time:  0.47791361808776855\n",
      "\t\tEpoch time:  610.5691804885864\n",
      "Epoch 19\n",
      "\t\tClassifier train accuracy:  0.998896579396128\n",
      "\t\tClassifier test accuracy 0.719921875\n",
      "\t\tStep time:  0.48053812980651855\n",
      "\t\tEpoch time:  613.1507942676544\n",
      "Epoch 20\n",
      "\t\tClassifier train accuracy:  0.9996990671080349\n",
      "\t\tClassifier test accuracy 0.7248592115848753\n",
      "\t\tStep time:  0.4762909412384033\n",
      "\t\tEpoch time:  610.9078440666199\n",
      "Epoch 21\n",
      "\t\tClassifier train accuracy:  0.9998996890360117\n",
      "\t\tClassifier test accuracy 0.7381335478680612\n",
      "\t\tStep time:  0.4798765182495117\n",
      "\t\tEpoch time:  610.5374193191528\n",
      "Epoch 22\n",
      "\t\tClassifier train accuracy:  0.9999498445180058\n",
      "\t\tClassifier test accuracy 0.7353177795655672\n",
      "\t\tStep time:  0.4741225242614746\n",
      "\t\tEpoch time:  610.8308391571045\n",
      "Epoch 23\n",
      "\t\tClassifier train accuracy:  0.9998996890360117\n",
      "\t\tClassifier test accuracy 0.7260659694288013\n",
      "\t\tStep time:  0.47731637954711914\n",
      "\t\tEpoch time:  611.6762638092041\n",
      "Epoch 24\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7228479485116653\n",
      "\t\tStep time:  0.4793837070465088\n",
      "\t\tEpoch time:  611.7002077102661\n",
      "Epoch 25\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7011263073209976\n",
      "\t\tStep time:  0.4825270175933838\n",
      "\t\tEpoch time:  610.8019630908966\n",
      "Epoch 26\n",
      "\t\tClassifier train accuracy:  0.9999498445180058\n",
      "\t\tClassifier test accuracy 0.7160096540627514\n",
      "\t\tStep time:  0.4688875675201416\n",
      "\t\tEpoch time:  611.2578701972961\n",
      "Epoch 27\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7035398230088495\n",
      "\t\tStep time:  0.46738266944885254\n",
      "\t\tEpoch time:  610.6202013492584\n",
      "Epoch 28\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7316975060337892\n",
      "\t\tStep time:  0.47450947761535645\n",
      "\t\tEpoch time:  610.7834091186523\n",
      "Epoch 29\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7397425583266292\n",
      "\t\tStep time:  0.4709320068359375\n",
      "\t\tEpoch time:  611.0975649356842\n",
      "Epoch 30\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7139983909895414\n",
      "\t\tStep time:  0.48115992546081543\n",
      "\t\tEpoch time:  611.0685453414917\n",
      "Epoch 31\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7156074014481094\n",
      "\t\tStep time:  0.4792964458465576\n",
      "\t\tEpoch time:  611.292366027832\n",
      "Epoch 32\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7139983909895414\n",
      "\t\tStep time:  0.4787735939025879\n",
      "\t\tEpoch time:  611.168229341507\n",
      "Epoch 33\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7357200321802092\n",
      "\t\tStep time:  0.4860262870788574\n",
      "\t\tEpoch time:  610.7274565696716\n",
      "Epoch 34\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7111826226870475\n",
      "\t\tStep time:  0.47145748138427734\n",
      "\t\tEpoch time:  611.3304369449615\n",
      "Epoch 35\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7288817377312953\n",
      "\t\tStep time:  0.4695558547973633\n",
      "\t\tEpoch time:  610.8280308246613\n",
      "Epoch 36\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7192276749798874\n",
      "\t\tStep time:  0.47298669815063477\n",
      "\t\tEpoch time:  610.4678723812103\n",
      "Epoch 37\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7164119066773934\n",
      "\t\tStep time:  0.48412394523620605\n",
      "\t\tEpoch time:  610.6971106529236\n",
      "Epoch 38\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7296862429605793\n",
      "\t\tStep time:  0.4685089588165283\n",
      "\t\tEpoch time:  610.5333633422852\n",
      "Epoch 39\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7260659694288013\n",
      "\t\tStep time:  0.4841725826263428\n",
      "\t\tEpoch time:  610.6396260261536\n",
      "Epoch 40\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.715234375\n",
      "\t\tStep time:  0.4781167507171631\n",
      "\t\tEpoch time:  612.494669675827\n",
      "Epoch 41\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7272727272727273\n",
      "\t\tStep time:  0.4730396270751953\n",
      "\t\tEpoch time:  610.7334804534912\n",
      "Epoch 42\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7139983909895414\n",
      "\t\tStep time:  0.47158145904541016\n",
      "\t\tEpoch time:  611.1421418190002\n",
      "Epoch 43\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7192276749798874\n",
      "\t\tStep time:  0.46820545196533203\n",
      "\t\tEpoch time:  610.4290547370911\n",
      "Epoch 44\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7252614641995173\n",
      "\t\tStep time:  0.4752984046936035\n",
      "\t\tEpoch time:  610.6160280704498\n",
      "Epoch 45\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7284794851166533\n",
      "\t\tStep time:  0.477083683013916\n",
      "\t\tEpoch time:  612.1894998550415\n",
      "Epoch 46\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7208366854384554\n",
      "\t\tStep time:  0.47638678550720215\n",
      "\t\tEpoch time:  612.2959399223328\n",
      "Epoch 47\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7260659694288013\n",
      "\t\tStep time:  0.481062650680542\n",
      "\t\tEpoch time:  612.1254730224609\n",
      "Epoch 48\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7135961383748994\n",
      "\t\tStep time:  0.48193788528442383\n",
      "\t\tEpoch time:  610.9027271270752\n",
      "Epoch 49\n",
      "\t\tClassifier train accuracy:  1.0\n",
      "\t\tClassifier test accuracy 0.7180209171359614\n",
      "\t\tStep time:  0.47596073150634766\n",
      "\t\tEpoch time:  610.3033294677734\n",
      "Epoch 50\n"
     ]
    }
   ],
   "source": [
    "# dataset = Dataset(trainset, testset)\n",
    "epochs = 77\n",
    "train_accuracies, test_accuracies, samples = train(net,epochs, s, batch_size, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
